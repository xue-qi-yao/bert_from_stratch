{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28, 26, 30, 19, 20, 35, 5],\n",
       " [28, 5, 18, 33, 22, 27, 14, 17, 15, 19],\n",
       " [14, 15, 19, 4, 26, 30, 19, 29],\n",
       " [9, 18, 34, 23, 36, 7, 24],\n",
       " [31, 38, 27],\n",
       " [11, 19, 5],\n",
       " [21, 30, 19, 16, 29],\n",
       " [20, 35, 16, 8, 37, 25, 19],\n",
       " [20, 35, 16, 17, 12, 18, 10, 6, 22, 13, 39, 32]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n' # R\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
    "    'Nice meet you too. How are you today?\\n' # R\n",
    "    'Great. My baseball team won the competition.\\n' # J\n",
    "    'Oh Congratulations, Juliet\\n' # R\n",
    "    'Thank you Romeo\\n' # J\n",
    "    'Where are you going today?\\n' # R\n",
    "    'I am going shopping. What about you?\\n' # J\n",
    "    'I am going to visit my grandmother. she is not very well' # R\n",
    ")\n",
    "\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')\n",
    "word_list = list(set(\" \".join(sentences).split(\" \")))\n",
    "word2idx = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, word in enumerate(word_list):\n",
    "    word2idx[word] = i + 4\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word2idx[word] for word in sentence.split(\" \")]\n",
    "    token_list.append(arr)\n",
    "\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30 # set the length of token number in a batch, padding with [PAD] token\n",
    "batch_size = 6\n",
    "max_pred = 5 # the max number for the prediction token in the masked token prediction task\n",
    "n_layers = 6 # the encoder layer number\n",
    "n_heads = 12 # the number of head in multihead attention\n",
    "d_model = 768 # the token embedding, segement embedding, position embedding dimension\n",
    "d_ff = d_model * 4 # the dimension of the FFN layer in the encoder layer\n",
    "d_kq = d_v = 64 # dimension of K, Q, V\n",
    "n_segments = 2 # the number of the sentence for encoder input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "* ##### The 15% of the token in the sentence nned to be replaced or masked in one sentence\n",
    "* ##### Two random sentences need to be concatenate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,\n",
       " [[1,\n",
       "   21,\n",
       "   30,\n",
       "   19,\n",
       "   3,\n",
       "   3,\n",
       "   2,\n",
       "   9,\n",
       "   18,\n",
       "   34,\n",
       "   23,\n",
       "   36,\n",
       "   7,\n",
       "   24,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  [29, 16, 0, 0, 0],\n",
       "  [5, 4, 0, 0, 0],\n",
       "  False])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_data():\n",
    "    batch = list()\n",
    "    positive, negative = 0, 0 \n",
    "    # postive: number of two continuous sentences\n",
    "    # negative: number of two not continuous sentences\n",
    "    # the ratio should be around 1:1\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        tokens_a_idx, tokens_b_idx = random.randint(0, len(sentences)-1), random.randint(0, len(sentences)-1)\n",
    "        tokens_a, tokens_b = token_list[tokens_a_idx], token_list[tokens_b_idx]\n",
    "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # masking\n",
    "        n_pred = min(max_pred, max(1, int(len(input_ids) * 0.15)))\n",
    "        token_maked_pos = [i for i, token in enumerate(input_ids) if token != word2idx['[CLS]'] and token != word2idx['[SEP]']]\n",
    "        random.shuffle(token_maked_pos)\n",
    "        masked_tokens, masked_pos = list(), list()\n",
    "        for pos in token_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random.random() < 0.8:\n",
    "                input_ids[pos] = word2idx['[MASK]']\n",
    "            elif random.random() > 0.9:\n",
    "                index = random.randint(4, vocab_size-1)\n",
    "                input_ids[pos] = index\n",
    "\n",
    "        # zero-padding for sentence\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # zero padding for mask\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_idx + 1 == tokens_b_idx and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_idx + 1 != tokens_b_idx and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "\n",
    "    return batch\n",
    "\n",
    "len(make_data()), make_data()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, is_next):\n",
    "          super().__init__()\n",
    "          self.input_ids = input_ids\n",
    "          self.segment_ids = segment_ids\n",
    "          self.masked_tokens = masked_tokens\n",
    "          self.masked_pos = masked_pos\n",
    "          self.is_next = is_next\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.is_next[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "         return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_data()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = zip(*batch)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(is_next)\n",
    "loader = DataLoader(MyDataset(input_ids, segment_ids, masked_tokens, masked_pos, is_next), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atten_pad_mask(seq_q):\n",
    "    batch_size, seq_len = seq_q.size()\n",
    "    pad_atten_mask = (seq_q == 0).unsqueeze(dim=-1) # (batch_size, seq_len, 1)\n",
    "    return pad_atten_mask.expand(batch_size, seq_len, seq_len)\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, atten_mask, d_kq):\n",
    "        scores = Q @ K.transpose(-1, -2) / d_kq**0.5\n",
    "        scores.masked_fill_(atten_mask, -1e9)\n",
    "        atten = F.softmax(scores, dim=-1)\n",
    "        context = atten @ V\n",
    "        return context\n",
    "\n",
    "class MutliHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_kq, d_v, n_heads):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(d_model, d_kq * n_heads)\n",
    "        self.wk = nn.Linear(d_model, d_kq * n_heads)\n",
    "        self.wv = nn.Linear(d_model, d_v * n_heads)\n",
    "        self.d_model = d_model\n",
    "        self.d_kq = d_kq\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, Q, K, V, atten_mask):\n",
    "        # Q, K, V: (batch_size, seq_len, d_model)\n",
    "        residual = Q\n",
    "        batch_size, seq_len, d_model = Q.shape\n",
    "        q_s = self.wq(Q).reshape(batch_size, -1, self.n_heads, self.d_kq).permute(0, 2, 1, 3)\n",
    "        k_s = self.wk(K).reshape(batch_size, -1, self.n_heads, self.d_kq).permute(0, 2, 1, 3)\n",
    "        v_s = self.wv(V).reshape(batch_size, -1, self.n_heads, self.d_v).permute(0, 2, 1, 3)\n",
    "        # q_s, k_s, v_s: (batch_size, n_heads, seq_len, d_qkv)\n",
    "\n",
    "        atten_mask = atten_mask.unsqueeze(dim=1).repeat(1, self.n_heads, 1, 1) # (batch_size, n_heads, seq_len, seq_len)\n",
    "\n",
    "        context = ScaledDotProductAttention()(q_s, k_s, v_s, atten_mask, self.d_kq)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, seq_len, self.n_heads * self.d_v)\n",
    "        output = nn.Linear(self.n_heads * self.d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual)\n",
    "    \n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc_atten = MutliHeadAttention(d_model, d_kq, d_v, n_heads)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "    def forward(self, enc_input, enc_atten_mask):\n",
    "        enc_output = self.enc_atten(enc_input, enc_input, enc_input, enc_atten_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output\n",
    "    \n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.shape[1]\n",
    "        pos = torch.arange(seq_len)\n",
    "        pos = pos.unsqueeze(dim=0).expand(*x.shape)\n",
    "        embedding = self.token_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.enc_atten = MutliHeadAttention(d_model, d_kq, d_v, n_heads)\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer() for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.d_model, d_model),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.gelu = nn.GELU()\n",
    "        embed_weight = self.embedding.token_embed.weight\n",
    "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.fc2.weight = embed_weight\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_atten_mask = get_atten_pad_mask(input_ids)\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, enc_atten_mask)\n",
    "        # get [CLS] token\n",
    "        h_pooled = self.fc(output[:, 0])\n",
    "        # for is_next prediction\n",
    "        logits_clsf = self.classifier(h_pooled)\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model)\n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "        h_masked = self.gelu(self.linear(h_masked))\n",
    "        logits_lm = self.fc2(h_masked)\n",
    "\n",
    "        return logits_clsf, logits_lm\n",
    "\n",
    "model = BERT(d_model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 loss = 1.447332\n",
      "Epoch: 0020 loss = 1.007936\n",
      "Epoch: 0030 loss = 0.873854\n",
      "Epoch: 0040 loss = 0.855126\n",
      "Epoch: 0050 loss = 0.875040\n",
      "Epoch: 0060 loss = 0.851540\n",
      "Epoch: 0070 loss = 0.885756\n",
      "Epoch: 0080 loss = 0.897899\n",
      "Epoch: 0090 loss = 0.799609\n",
      "Epoch: 0100 loss = 0.883441\n",
      "Epoch: 0110 loss = 0.860890\n",
      "Epoch: 0120 loss = 0.818617\n",
      "Epoch: 0130 loss = 0.829771\n",
      "Epoch: 0140 loss = 0.803633\n",
      "Epoch: 0150 loss = 0.789039\n",
      "Epoch: 0160 loss = 0.897564\n",
      "Epoch: 0170 loss = 0.775942\n",
      "Epoch: 0180 loss = 0.852835\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(180):\n",
    "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
    "      logits_clsf, logits_lm = model(input_ids, segment_ids, masked_pos)\n",
    "      loss_lm = loss_fn(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n",
    "      loss_lm = (loss_lm.float()).mean()\n",
    "      loss_clsf = loss_fn(logits_clsf, isNext) # for sentence classification\n",
    "      loss = loss_lm + loss_clsf\n",
    "      if (epoch + 1) % 10 == 0:\n",
    "          print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thank you Romeo\n",
      "Where are you going today?\n",
      "I am going shopping. What about you?\n",
      "I am going to visit my grandmother. she is not very well\n",
      "['[CLS]', 'thank', 'you', 'romeo', '[SEP]', 'great', 'my', 'baseball', 'team', 'won', 'hello', 'competition', '[SEP]']\n",
      "masked tokens list :  [7]\n",
      "predict masked tokens list :  [7]\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[0]\n",
    "print(text)\n",
    "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
    "\n",
    "logits_clsf, logits_lm = model(torch.LongTensor([input_ids]), \\\n",
    "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CogVideoX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
